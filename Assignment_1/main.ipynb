{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae674b3-87b5-4df4-8503-3bd6d45720f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e9d573-ab56-4c6e-853d-6e5ecee18e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import git\n",
    "import ast\n",
    "import logging\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys # For getting current exception info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603a166e-e533-4429-9463-542c9a75af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "INPUT_SEART_CSV = \"seart_repos.csv\" \n",
    "OUTPUT_REPO_LIST = \"repositories.txt\"\n",
    "\n",
    "REPO_LIST_FILE = \"repositories.txt\" \n",
    "CLONE_DIR = \"repos\"\n",
    "\n",
    "# Define a dedicated output data directory \n",
    "OUTPUT_DATA_DIR = \"collected_data\" # All CSVs and description files will go here\n",
    "\n",
    "# Define your output CSV and description file paths relative to OUTPUT_DATA_DIR\n",
    "OUTPUT_CSV_FILE = os.path.join(OUTPUT_DATA_DIR, \"python_functions.csv\")\n",
    "DESCRIPTION_FILE = os.path.join(OUTPUT_DATA_DIR, \"dataset_description.txt\")\n",
    "\n",
    "MAX_FUNCTIONS = 250000 \n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f45c4-12f4-44dd-a814-22f826626e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonFunctionExtractor(ast.NodeVisitor):\n",
    "    \"\"\"\n",
    "    An AST visitor that extracts the source code and metadata of Python function definitions.\n",
    "    \"\"\"\n",
    "    def __init__(self, full_source_code): # <-- Renamed parameter for clarity\n",
    "        self.full_source_code = full_source_code # <-- Store the full string\n",
    "        self.functions = []\n",
    "        \n",
    "    def visit_FunctionDef(self, node):\n",
    "        \"\"\"Called for 'def' function definitions.\"\"\"\n",
    "        logging.debug(f\"DEBUG: Found FunctionDef: {node.name} at line {node.lineno}\") # ADD THIS DEBUG LOG\n",
    "        self._extract_function_info(node)\n",
    "        self.generic_visit(node) # Continue for nested functions\n",
    "\n",
    "    def visit_AsyncFunctionDef(self, node):\n",
    "        \"\"\"Called for 'async def' function definitions.\"\"\"\n",
    "        logging.debug(f\"DEBUG: Found AsyncFunctionDef: {node.name} at line {node.lineno}\") # ADD THIS DEBUG LOG\n",
    "        self._extract_function_info(node)\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def _extract_function_info(self, node):\n",
    "        try:\n",
    "            # Pass the original full source code string here\n",
    "            original_code = ast.get_source_segment(source=self.full_source_code, node=node) # <-- CHANGE IS HERE\n",
    "            if original_code:\n",
    "                # ast.FunctionDef provides lineno and end_lineno (Python 3.8+)\n",
    "                start_line = node.lineno\n",
    "                end_line = getattr(node, 'end_lineno', start_line + original_code.count('\\n')) \n",
    "                \n",
    "                signature = f\"def {node.name}(...)\" \n",
    "                if isinstance(node, ast.AsyncFunctionDef):\n",
    "                    signature = f\"async {signature}\"\n",
    "\n",
    "                self.functions.append({\n",
    "                    \"function_name\": node.name,\n",
    "                    \"start_line\": start_line,\n",
    "                    \"end_line\": end_line,\n",
    "                    \"signature\": signature,\n",
    "                    \"original_code\": original_code,\n",
    "                    \"code_tokens\": \" \".join(original_code.split())\n",
    "                })\n",
    "                logging.debug(f\"DEBUG: Successfully extracted source for '{node.name}'. Length: {len(original_code)} chars.\")\n",
    "            else:\n",
    "                logging.debug(f\"DEBUG: ast.get_source_segment returned empty for '{node.name}'.\")\n",
    "        except (TypeError, ValueError) as e:\n",
    "            logging.warning(f\"Could not extract source for a function at line {node.lineno}: {e}\")\n",
    "        except Exception:\n",
    "            logging.error(f\"Unexpected error extracting function at line {node.lineno}: {sys.exc_info()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686aaf39-0557-456e-8e4e-65148b354e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_functions(function_info, counters):\n",
    "    \"\"\"\n",
    "    Applies cleaning rules to a dictionary representing a function.\n",
    "    Updates counters for removed functions.\n",
    "    \"\"\"\n",
    "    start, end = function_info.get(\"start_line\"), function_info.get(\"end_line\")\n",
    "    code = function_info.get(\"original_code\", \"\")\n",
    "\n",
    "    if start is None or end is None or (isinstance(start, int) and isinstance(end, int) and start > end):\n",
    "        counters['invalid_lines'] += 1\n",
    "        return None\n",
    "    \n",
    "    # Consider the actual number of lines of code, excluding leading/trailing whitespace lines\n",
    "    code_lines = [line.strip() for line in code.splitlines() if line.strip()]\n",
    "    num_lines = len(code_lines)\n",
    "\n",
    "    if num_lines < 3: # Minimum 3 lines of *actual* code\n",
    "        counters['short_functions'] += 1\n",
    "        return None\n",
    "    if num_lines > 100: # Maximum 100 lines of *actual* code\n",
    "        counters['long_functions'] += 1\n",
    "        return None\n",
    "    if not code.strip(): # Check if code is empty after stripping whitespace\n",
    "        counters['empty_functions'] += 1\n",
    "        return None\n",
    "    \n",
    "    # Optional: Filter out functions with very high comment-to-code ratio if desired\n",
    "    # For now, we'll keep them as comments are part of code context.\n",
    "\n",
    "    return function_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc0b08-193f-454c-a2bb-130710098392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_repositories():\n",
    "    \"\"\"\n",
    "    Clones repositories from the list specified in REPO_LIST_FILE.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(REPO_LIST_FILE):\n",
    "        logging.error(f\"'{REPO_LIST_FILE}' not found. Please create it and add repository URLs.\")\n",
    "        return False\n",
    "\n",
    "    if not os.path.exists(CLONE_DIR):\n",
    "        logging.info(f\"Creating directory for repositories: '{CLONE_DIR}'\")\n",
    "        os.makedirs(CLONE_DIR)\n",
    "\n",
    "    repos_to_process = []\n",
    "    with open(REPO_LIST_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            repo_url = line.strip()\n",
    "            if repo_url and not repo_url.startswith('#'):\n",
    "                repos_to_process.append(repo_url)\n",
    "    \n",
    "    if not repos_to_process:\n",
    "        logging.warning(f\"No repository URLs found in '{REPO_LIST_FILE}'. Please add some.\")\n",
    "        return False\n",
    "\n",
    "    for repo_url in repos_to_process:\n",
    "        try:\n",
    "            repo_name = repo_url.split('/')[-1].replace('.git', '')\n",
    "            local_path = os.path.join(CLONE_DIR, repo_name)\n",
    "\n",
    "            if os.path.exists(local_path):\n",
    "                logging.info(f\"Repository '{repo_name}' already exists. Skipping clone.\")\n",
    "            else:\n",
    "                logging.info(f\"Cloning '{repo_url}' into '{local_path}'...\")\n",
    "                git.Repo.clone_from(repo_url, local_path)\n",
    "                logging.info(f\"Successfully cloned '{repo_name}'.\")\n",
    "        except git.exc.GitCommandError as e:\n",
    "            logging.error(f\"Failed to clone {repo_url}: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An unexpected error occurred for {repo_url}: {e}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c8432-2850-4b0a-bd91-c0eb00763187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cloned_repositories():\n",
    "    \"\"\"\n",
    "    Walks through cloned repos, extracts, cleans Python functions, and writes to CSV.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(OUTPUT_CSV_FILE), exist_ok=True)\n",
    "    \n",
    "    # Initialize counters for cleaning statistics\n",
    "    counters = {\n",
    "        'total_scanned_files': 0,\n",
    "        'total_parsed_files': 0,\n",
    "        'total_skipped_files_syntax_error': 0,\n",
    "        'total_skipped_files_other_error': 0,\n",
    "        'total_extracted_functions_raw': 0,\n",
    "        'short_functions': 0,\n",
    "        'long_functions': 0,\n",
    "        'empty_functions': 0,\n",
    "        'invalid_lines': 0,\n",
    "        'total_written_functions_cleaned': 0\n",
    "    }\n",
    "\n",
    "    # Set up CSV writer, managing header creation\n",
    "    csv_file_exists = os.path.exists(OUTPUT_CSV_FILE) and os.path.getsize(OUTPUT_CSV_FILE) > 0\n",
    "    fieldnames = [\n",
    "        \"repo_name\", \"repo_url\", \"file_path\", \"function_name\", \"start_line\",\n",
    "        \"end_line\", \"signature\", \"original_code\", \"code_tokens\"\n",
    "    ]\n",
    "    \n",
    "    f_csv = open(OUTPUT_CSV_FILE, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "    writer = csv.DictWriter(f_csv, fieldnames=fieldnames)\n",
    "    if not csv_file_exists:\n",
    "        writer.writeheader()\n",
    "\n",
    "    seen_function_hashes = set() # To store hashes for deduplication\n",
    "\n",
    "    logging.info(f\"Starting function extraction and writing to '{OUTPUT_CSV_FILE}'...\")\n",
    "\n",
    "    for root, _, files in os.walk(CLONE_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Determine repo_name and repo_url from the file_path\n",
    "                relative_path = os.path.relpath(file_path, CLONE_DIR)\n",
    "                repo_name_guess = relative_path.split(os.sep)[0]\n",
    "                # A more robust way might be to pass repo_url from clone_repositories\n",
    "                # For simplicity here, we'll try to reconstruct.\n",
    "                repo_url_guess = \"https://github.com/unknown/unknown\" # Placeholder, improve if needed\n",
    "\n",
    "                # Get the actual repo URL from REPO_LIST_FILE based on repo_name_guess\n",
    "                with open(REPO_LIST_FILE, 'r') as rlf:\n",
    "                    for line in rlf:\n",
    "                        if repo_name_guess in line:\n",
    "                            repo_url_guess = line.strip()\n",
    "                            break\n",
    "\n",
    "\n",
    "                counters['total_scanned_files'] += 1\n",
    "                if counters['total_scanned_files'] % 1000 == 0:\n",
    "                    logging.info(f\"Scanned {counters['total_scanned_files']} Python files...\")\n",
    "                    logging.info(f\"Currently extracted and cleaned: {counters['total_written_functions_cleaned']} functions.\")\n",
    "                \n",
    "                if counters['total_written_functions_cleaned'] >= MAX_FUNCTIONS:\n",
    "                    logging.info(f\"Reached MAX_FUNCTIONS ({MAX_FUNCTIONS}). Stopping collection.\")\n",
    "                    f_csv.close()\n",
    "                    return counters # Early exit\n",
    "\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        file_content = f.read()\n",
    "                    \n",
    "                    tree = ast.parse(file_content)\n",
    "                    counters['total_parsed_files'] += 1\n",
    "                    \n",
    "                    extractor = PythonFunctionExtractor(file_content)\n",
    "                    extractor.visit(tree)\n",
    "\n",
    "                    for func_data in extractor.functions:\n",
    "                        counters['total_extracted_functions_raw'] += 1\n",
    "                        \n",
    "                        # Add repo and file path metadata\n",
    "                        full_func_info = {\n",
    "                            \"repo_name\": repo_name_guess,\n",
    "                            \"repo_url\": repo_url_guess,\n",
    "                            \"file_path\": os.path.relpath(file_path, os.path.join(CLONE_DIR, repo_name_guess)),\n",
    "                            **func_data\n",
    "                        }\n",
    "\n",
    "                        cleaned_func = clean_functions(full_func_info, counters)\n",
    "\n",
    "                        if cleaned_func:\n",
    "                            # Deduplicate based on a hash of the cleaned code content\n",
    "                            # This catches exact duplicates across files/repos\n",
    "                            func_hash = hash(cleaned_func['original_code'])\n",
    "                            if func_hash not in seen_function_hashes:\n",
    "                                writer.writerow(cleaned_func)\n",
    "                                seen_function_hashes.add(func_hash)\n",
    "                                counters['total_written_functions_cleaned'] += 1\n",
    "                            else:\n",
    "                                # Not explicitly counted, but implicitly removed by set check\n",
    "                                pass \n",
    "                            \n",
    "                            if counters['total_written_functions_cleaned'] >= MAX_FUNCTIONS:\n",
    "                                logging.info(f\"Reached MAX_FUNCTIONS ({MAX_FUNCTIONS}). Stopping collection.\")\n",
    "                                f_csv.close()\n",
    "                                return counters # Early exit\n",
    "\n",
    "                except SyntaxError:\n",
    "                    logging.warning(f\"Skipping {file_path} due to SyntaxError.\")\n",
    "                    counters['total_skipped_files_syntax_error'] += 1\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"An unexpected error occurred processing {file_path}: {e}\")\n",
    "                    counters['total_skipped_files_other_error'] += 1\n",
    "    \n",
    "    f_csv.close() # Ensure CSV file is closed at the end of successful run\n",
    "    logging.info(\"Function extraction and writing to CSV complete.\")\n",
    "    return counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3d363-403c-42de-9149-bc13b33ab1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description_file(counters):\n",
    "    \"\"\"\n",
    "    Creates a summary description file for the collected dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(DESCRIPTION_FILE), exist_ok=True)\n",
    "    \n",
    "    description_text = f\"\"\"\n",
    "    Dataset Collection Summary (Python Functions):\n",
    "\n",
    "    Configuration:\n",
    "    - Repository List: {REPO_LIST_FILE}\n",
    "    - Clone Directory: {CLONE_DIR}\n",
    "    - Output CSV: {OUTPUT_CSV_FILE}\n",
    "    - Maximum Functions Target: {MAX_FUNCTIONS}\n",
    "\n",
    "    Processing Statistics:\n",
    "    - Total Python files scanned: {counters['total_scanned_files']}\n",
    "    - Total Python files successfully parsed (AST): {counters['total_parsed_files']}\n",
    "    - Files skipped due to SyntaxError: {counters['total_skipped_files_syntax_error']}\n",
    "    - Files skipped due to other errors: {counters['total_skipped_files_other_error']}\n",
    "\n",
    "    Function Extraction Statistics (Raw):\n",
    "    - Total functions initially extracted (before cleaning): {counters['total_extracted_functions_raw']}\n",
    "\n",
    "    Function Cleaning Statistics:\n",
    "    - Functions removed:\n",
    "        - Too short (<3 lines of actual code): {counters['short_functions']}\n",
    "        - Too long (>100 lines of actual code): {counters['long_functions']}\n",
    "        - Empty (after stripping whitespace): {counters['empty_functions']}\n",
    "        - Invalid line numbers/structure: {counters['invalid_lines']}\n",
    "        - Duplicates (based on code hash): {counters['total_extracted_functions_raw'] - counters['total_written_functions_cleaned'] - (counters['short_functions'] + counters['long_functions'] + counters['empty_functions'] + counters['invalid_lines'])}\n",
    "          (This is an estimate of only *deduplicated* functions that passed other filters. The true duplicate count is more complex to show here.)\n",
    "          \n",
    "    Final Dataset:\n",
    "    - Total unique, cleaned functions written to CSV: {counters['total_written_functions_cleaned']}\n",
    "    \n",
    "    Each row in '{OUTPUT_CSV_FILE}' represents a cleaned Python function and includes:\n",
    "    repo_name, repo_url, file_path, function_name, start_line, end_line, signature, original_code, code_tokens (simple split).\n",
    "    \"\"\"\n",
    "\n",
    "    with open(DESCRIPTION_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(description_text)\n",
    "    logging.info(f\"Dataset description saved to '{DESCRIPTION_FILE}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2193ab-a418-4d1b-abd9-c177ea056c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_repo_list_from_seart_csv(input_csv, output_txt):\n",
    "    if not os.path.exists(input_csv):\n",
    "        print(f\"Error: {input_csv} not found.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Read the CSV. The 'name' column appears to be in the format 'owner/repo'\n",
    "        df = pd.read_csv(input_csv)\n",
    "        \n",
    "        # Ensure the 'name' column exists\n",
    "        if 'name' not in df.columns:\n",
    "            print(f\"Error: '{input_csv}' does not contain a 'name' column.\")\n",
    "            return\n",
    "\n",
    "        # Generate GitHub URLs\n",
    "        github_urls = [f\"https://github.com/{repo_name}\" for repo_name in df['name'].unique()]\n",
    "\n",
    "        with open(output_txt, 'w') as f:\n",
    "            for url in github_urls:\n",
    "                f.write(url + \"\\n\")\n",
    "        print(f\"Successfully generated '{output_txt}' with {len(github_urls)} URLs.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6d2d8-c7c8-4880-846e-dc12a89339af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b49b1-fa78-49a1-a1c8-2a0d0f94eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the code and repo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Ensure the main output data directory exists\n",
    "    if not os.path.exists(OUTPUT_DATA_DIR):\n",
    "        logging.info(f\"Creating output data directory: '{OUTPUT_DATA_DIR}'\")\n",
    "        os.makedirs(OUTPUT_DATA_DIR)\n",
    "\n",
    "    # 2. Clone repositories\n",
    "    if clone_repositories():\n",
    "        # 3. Process cloned repositories, extract functions, clean, and write to CSV\n",
    "        final_counters = process_cloned_repositories()\n",
    "        \n",
    "        # 4. Generate a description file based on collected statistics\n",
    "        generate_description_file(final_counters)\n",
    "    else:\n",
    "        logging.error(\"Repository cloning step failed. Please check your 'repositories.txt' and internet connection.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
