{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_DATA_DIR = \"collected_data\"\n",
    "INPUT_CSV = os.path.join(OUTPUT_DATA_DIR, \"python_functions.csv\")\n",
    "TOKENIZER_TRAIN_CORPUS_FILE = os.path.join(OUTPUT_DATA_DIR, \"tokenizer_train_corpus.txt\")\n",
    "\n",
    "TOKENIZER_DIR = \"tokenizer_files\"\n",
    "TOKENIZER_JSON_PATH = os.path.join(TOKENIZER_DIR, \"tokenizer.json\") # We will now save to a single .json file\n",
    "\n",
    "# --- Best Settings for Tokenizer Training ---\n",
    "VOCAB_SIZE = 40000\n",
    "MIN_FREQUENCY = 3\n",
    "SPECIAL_TOKENS = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\", \"<if_mask>\"]\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def prepare_tokenizer_corpus():\n",
    "    if not os.path.exists(INPUT_CSV):\n",
    "        logging.error(f\"Input CSV file not found: {INPUT_CSV}. Please run collect_data.py first.\")\n",
    "        return False\n",
    "    logging.info(f\"Loading '{INPUT_CSV}' to prepare tokenizer training corpus...\")\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    with open(TOKENIZER_TRAIN_CORPUS_FILE, 'w', encoding='utf-8') as f:\n",
    "        for code in df['original_code']:\n",
    "            f.write(str(code) + \"\\n\")\n",
    "    logging.info(f\"Corpus prepared at '{TOKENIZER_TRAIN_CORPUS_FILE}'.\")\n",
    "    return True\n",
    "\n",
    "def train_custom_tokenizer():\n",
    "    \"\"\"\n",
    "    Trains a Byte-Level BPE tokenizer and saves it to a single tokenizer.json file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(TOKENIZER_TRAIN_CORPUS_FILE):\n",
    "        logging.error(f\"Corpus file not found: {TOKENIZER_TRAIN_CORPUS_FILE}.\")\n",
    "        return False\n",
    "\n",
    "    # 1. Initialize a new Tokenizer with a BPE model\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "\n",
    "    # 2. Configure Byte-Level behavior\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    # 3. Initialize a trainer\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        min_frequency=MIN_FREQUENCY,\n",
    "        special_tokens=SPECIAL_TOKENS\n",
    "    )\n",
    "\n",
    "    # 4. Train the tokenizer\n",
    "    logging.info(\"Starting tokenizer training...\")\n",
    "    tokenizer.train([TOKENIZER_TRAIN_CORPUS_FILE], trainer)\n",
    "    logging.info(\"Tokenizer training complete.\")\n",
    "\n",
    "    # 5. Save the tokenizer to a single, robust tokenizer.json file\n",
    "    if not os.path.exists(TOKENIZER_DIR):\n",
    "        os.makedirs(TOKENIZER_DIR)\n",
    "    tokenizer.save(TOKENIZER_JSON_PATH)\n",
    "    logging.info(f\"Tokenizer saved successfully to '{TOKENIZER_JSON_PATH}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 16:23:04,885 - INFO - Loading 'collected_data/python_functions.csv' to prepare tokenizer training corpus...\n",
      "2025-10-12 16:23:07,742 - INFO - Corpus prepared at 'collected_data/tokenizer_train_corpus.txt'.\n",
      "2025-10-12 16:23:07,782 - INFO - Starting tokenizer training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 16:23:22,822 - INFO - Tokenizer training complete.\n",
      "2025-10-12 16:23:22,834 - INFO - Tokenizer saved successfully to 'tokenizer_files/tokenizer.json'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if prepare_tokenizer_corpus():\n",
    "        train_custom_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
