{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 75000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 2.2695815563201904,
      "learning_rate": 4.966733333333334e-05,
      "loss": 7.3866,
      "step": 500
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 2.428152322769165,
      "learning_rate": 4.9334000000000006e-05,
      "loss": 6.2489,
      "step": 1000
    },
    {
      "epoch": 0.02,
      "grad_norm": 5.774367809295654,
      "learning_rate": 4.9000666666666667e-05,
      "loss": 6.0763,
      "step": 1500
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 4.400723934173584,
      "learning_rate": 4.8667333333333334e-05,
      "loss": 5.83,
      "step": 2000
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 8.240400314331055,
      "learning_rate": 4.8334e-05,
      "loss": 5.5568,
      "step": 2500
    },
    {
      "epoch": 0.04,
      "grad_norm": 8.490134239196777,
      "learning_rate": 4.800066666666667e-05,
      "loss": 5.3135,
      "step": 3000
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 6.7483601570129395,
      "learning_rate": 4.766733333333334e-05,
      "loss": 5.2151,
      "step": 3500
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 6.335896968841553,
      "learning_rate": 4.7334e-05,
      "loss": 5.0662,
      "step": 4000
    },
    {
      "epoch": 0.06,
      "grad_norm": 4.629440784454346,
      "learning_rate": 4.700066666666667e-05,
      "loss": 4.924,
      "step": 4500
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 5.432921886444092,
      "learning_rate": 4.666733333333333e-05,
      "loss": 4.8524,
      "step": 5000
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 9.104883193969727,
      "learning_rate": 4.6334e-05,
      "loss": 4.6979,
      "step": 5500
    },
    {
      "epoch": 0.08,
      "grad_norm": 9.337703704833984,
      "learning_rate": 4.600066666666667e-05,
      "loss": 4.5244,
      "step": 6000
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 12.502408027648926,
      "learning_rate": 4.5667333333333336e-05,
      "loss": 4.5809,
      "step": 6500
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 8.387307167053223,
      "learning_rate": 4.5334e-05,
      "loss": 4.3632,
      "step": 7000
    },
    {
      "epoch": 0.1,
      "grad_norm": 12.446344375610352,
      "learning_rate": 4.500066666666667e-05,
      "loss": 4.338,
      "step": 7500
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 9.647412300109863,
      "learning_rate": 4.466733333333333e-05,
      "loss": 4.2104,
      "step": 8000
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 12.820964813232422,
      "learning_rate": 4.4334000000000006e-05,
      "loss": 4.153,
      "step": 8500
    },
    {
      "epoch": 0.12,
      "grad_norm": 5.434669017791748,
      "learning_rate": 4.400066666666667e-05,
      "loss": 3.9991,
      "step": 9000
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 15.559184074401855,
      "learning_rate": 4.3667333333333335e-05,
      "loss": 3.9582,
      "step": 9500
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 19.545928955078125,
      "learning_rate": 4.3334e-05,
      "loss": 3.777,
      "step": 10000
    },
    {
      "epoch": 0.14,
      "grad_norm": 12.564620971679688,
      "learning_rate": 4.300066666666667e-05,
      "loss": 3.7369,
      "step": 10500
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 8.342878341674805,
      "learning_rate": 4.266733333333334e-05,
      "loss": 3.6753,
      "step": 11000
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 12.351396560668945,
      "learning_rate": 4.2334e-05,
      "loss": 3.6502,
      "step": 11500
    },
    {
      "epoch": 0.16,
      "grad_norm": 21.8151798248291,
      "learning_rate": 4.2000666666666666e-05,
      "loss": 3.5698,
      "step": 12000
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 23.103233337402344,
      "learning_rate": 4.1667333333333333e-05,
      "loss": 3.4658,
      "step": 12500
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 11.179431915283203,
      "learning_rate": 4.1334e-05,
      "loss": 3.3764,
      "step": 13000
    },
    {
      "epoch": 0.18,
      "grad_norm": 13.662700653076172,
      "learning_rate": 4.100066666666667e-05,
      "loss": 3.3345,
      "step": 13500
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 8.317983627319336,
      "learning_rate": 4.0667333333333336e-05,
      "loss": 3.2535,
      "step": 14000
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 7.377893924713135,
      "learning_rate": 4.0334e-05,
      "loss": 3.2211,
      "step": 14500
    },
    {
      "epoch": 0.2,
      "grad_norm": 13.693534851074219,
      "learning_rate": 4.000066666666667e-05,
      "loss": 3.1494,
      "step": 15000
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 20.22423553466797,
      "learning_rate": 3.966733333333333e-05,
      "loss": 3.0705,
      "step": 15500
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 7.998298645019531,
      "learning_rate": 3.9334000000000007e-05,
      "loss": 3.1289,
      "step": 16000
    },
    {
      "epoch": 0.22,
      "grad_norm": 10.602961540222168,
      "learning_rate": 3.900066666666667e-05,
      "loss": 2.9834,
      "step": 16500
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 12.747194290161133,
      "learning_rate": 3.8667333333333335e-05,
      "loss": 2.9959,
      "step": 17000
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 10.547789573669434,
      "learning_rate": 3.8334e-05,
      "loss": 3.0156,
      "step": 17500
    },
    {
      "epoch": 0.24,
      "grad_norm": 10.345142364501953,
      "learning_rate": 3.800066666666667e-05,
      "loss": 2.8745,
      "step": 18000
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 7.523201942443848,
      "learning_rate": 3.766733333333334e-05,
      "loss": 2.8429,
      "step": 18500
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 32.81443786621094,
      "learning_rate": 3.7334000000000005e-05,
      "loss": 2.8568,
      "step": 19000
    },
    {
      "epoch": 0.26,
      "grad_norm": 9.452169418334961,
      "learning_rate": 3.7000666666666666e-05,
      "loss": 2.8072,
      "step": 19500
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 10.055001258850098,
      "learning_rate": 3.6667333333333334e-05,
      "loss": 2.7678,
      "step": 20000
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 9.4539794921875,
      "learning_rate": 3.6334e-05,
      "loss": 2.6536,
      "step": 20500
    },
    {
      "epoch": 0.28,
      "grad_norm": 5.8626604080200195,
      "learning_rate": 3.600066666666667e-05,
      "loss": 2.7856,
      "step": 21000
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 10.170402526855469,
      "learning_rate": 3.566733333333334e-05,
      "loss": 2.6199,
      "step": 21500
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 10.100055694580078,
      "learning_rate": 3.5334e-05,
      "loss": 2.6505,
      "step": 22000
    },
    {
      "epoch": 0.3,
      "grad_norm": 14.034367561340332,
      "learning_rate": 3.500066666666667e-05,
      "loss": 2.6325,
      "step": 22500
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 10.005380630493164,
      "learning_rate": 3.466733333333333e-05,
      "loss": 2.6379,
      "step": 23000
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 9.187047958374023,
      "learning_rate": 3.4334e-05,
      "loss": 2.543,
      "step": 23500
    },
    {
      "epoch": 0.32,
      "grad_norm": 6.081852912902832,
      "learning_rate": 3.400066666666667e-05,
      "loss": 2.545,
      "step": 24000
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 7.964369773864746,
      "learning_rate": 3.3667333333333335e-05,
      "loss": 2.5328,
      "step": 24500
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 10.464058876037598,
      "learning_rate": 3.3334e-05,
      "loss": 2.5971,
      "step": 25000
    },
    {
      "epoch": 0.34,
      "grad_norm": 7.148861408233643,
      "learning_rate": 3.300066666666667e-05,
      "loss": 2.5038,
      "step": 25500
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 7.602145195007324,
      "learning_rate": 3.266733333333333e-05,
      "loss": 2.541,
      "step": 26000
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 4.4163947105407715,
      "learning_rate": 3.2334000000000006e-05,
      "loss": 2.5407,
      "step": 26500
    },
    {
      "epoch": 0.36,
      "grad_norm": 7.9833760261535645,
      "learning_rate": 3.200066666666667e-05,
      "loss": 2.4466,
      "step": 27000
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 39.87510299682617,
      "learning_rate": 3.1667333333333334e-05,
      "loss": 2.3896,
      "step": 27500
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 8.17542552947998,
      "learning_rate": 3.1334e-05,
      "loss": 2.4277,
      "step": 28000
    },
    {
      "epoch": 0.38,
      "grad_norm": 7.015267848968506,
      "learning_rate": 3.100066666666667e-05,
      "loss": 2.4117,
      "step": 28500
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 5.431166172027588,
      "learning_rate": 3.066733333333334e-05,
      "loss": 2.412,
      "step": 29000
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 7.659692287445068,
      "learning_rate": 3.0334e-05,
      "loss": 2.421,
      "step": 29500
    },
    {
      "epoch": 0.4,
      "grad_norm": 8.9920015335083,
      "learning_rate": 3.0000666666666666e-05,
      "loss": 2.3357,
      "step": 30000
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 6.194471836090088,
      "learning_rate": 2.9667333333333336e-05,
      "loss": 2.322,
      "step": 30500
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 12.831755638122559,
      "learning_rate": 2.9334e-05,
      "loss": 2.3505,
      "step": 31000
    },
    {
      "epoch": 0.42,
      "grad_norm": 16.645999908447266,
      "learning_rate": 2.900066666666667e-05,
      "loss": 2.2518,
      "step": 31500
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 13.361170768737793,
      "learning_rate": 2.8667333333333336e-05,
      "loss": 2.3049,
      "step": 32000
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 17.350574493408203,
      "learning_rate": 2.8334e-05,
      "loss": 2.3166,
      "step": 32500
    },
    {
      "epoch": 0.44,
      "grad_norm": 7.36846399307251,
      "learning_rate": 2.800066666666667e-05,
      "loss": 2.3304,
      "step": 33000
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 16.19182777404785,
      "learning_rate": 2.7667333333333335e-05,
      "loss": 2.2525,
      "step": 33500
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 12.47211742401123,
      "learning_rate": 2.7334000000000003e-05,
      "loss": 2.1812,
      "step": 34000
    },
    {
      "epoch": 0.46,
      "grad_norm": 6.77617073059082,
      "learning_rate": 2.7000666666666667e-05,
      "loss": 2.1933,
      "step": 34500
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 10.58554458618164,
      "learning_rate": 2.6667333333333335e-05,
      "loss": 2.2844,
      "step": 35000
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 6.3437113761901855,
      "learning_rate": 2.6334000000000002e-05,
      "loss": 2.2065,
      "step": 35500
    },
    {
      "epoch": 0.48,
      "grad_norm": 9.76145076751709,
      "learning_rate": 2.6000666666666667e-05,
      "loss": 2.2456,
      "step": 36000
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 4.5183424949646,
      "learning_rate": 2.566733333333333e-05,
      "loss": 2.1704,
      "step": 36500
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 7.7204484939575195,
      "learning_rate": 2.5334000000000002e-05,
      "loss": 2.1354,
      "step": 37000
    },
    {
      "epoch": 0.5,
      "grad_norm": 11.861520767211914,
      "learning_rate": 2.5000666666666666e-05,
      "loss": 2.2039,
      "step": 37500
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 14.429521560668945,
      "learning_rate": 2.4667333333333334e-05,
      "loss": 2.1417,
      "step": 38000
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 5.292019844055176,
      "learning_rate": 2.4334e-05,
      "loss": 2.2091,
      "step": 38500
    },
    {
      "epoch": 0.52,
      "grad_norm": 5.749056339263916,
      "learning_rate": 2.400066666666667e-05,
      "loss": 2.1268,
      "step": 39000
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 13.22513484954834,
      "learning_rate": 2.3667333333333336e-05,
      "loss": 2.1039,
      "step": 39500
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 9.938255310058594,
      "learning_rate": 2.3334e-05,
      "loss": 2.1256,
      "step": 40000
    },
    {
      "epoch": 0.54,
      "grad_norm": 5.9161577224731445,
      "learning_rate": 2.3000666666666668e-05,
      "loss": 2.1245,
      "step": 40500
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 3.950737476348877,
      "learning_rate": 2.2667333333333336e-05,
      "loss": 2.0681,
      "step": 41000
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 9.41873836517334,
      "learning_rate": 2.2334000000000003e-05,
      "loss": 2.1342,
      "step": 41500
    },
    {
      "epoch": 0.56,
      "grad_norm": 13.888189315795898,
      "learning_rate": 2.2000666666666668e-05,
      "loss": 2.1383,
      "step": 42000
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 7.63388729095459,
      "learning_rate": 2.1667333333333335e-05,
      "loss": 2.0566,
      "step": 42500
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 7.342062950134277,
      "learning_rate": 2.1334000000000003e-05,
      "loss": 2.1109,
      "step": 43000
    },
    {
      "epoch": 0.58,
      "grad_norm": 7.2044878005981445,
      "learning_rate": 2.1000666666666667e-05,
      "loss": 2.0591,
      "step": 43500
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 7.827444553375244,
      "learning_rate": 2.0667333333333335e-05,
      "loss": 2.025,
      "step": 44000
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 8.185075759887695,
      "learning_rate": 2.0334e-05,
      "loss": 2.0247,
      "step": 44500
    },
    {
      "epoch": 0.6,
      "grad_norm": 12.050613403320312,
      "learning_rate": 2.0000666666666666e-05,
      "loss": 2.0431,
      "step": 45000
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 8.408929824829102,
      "learning_rate": 1.9667333333333334e-05,
      "loss": 2.056,
      "step": 45500
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 13.387627601623535,
      "learning_rate": 1.9334e-05,
      "loss": 1.9656,
      "step": 46000
    },
    {
      "epoch": 0.62,
      "grad_norm": 8.582967758178711,
      "learning_rate": 1.9000666666666666e-05,
      "loss": 2.0576,
      "step": 46500
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 3.7939069271087646,
      "learning_rate": 1.8667333333333333e-05,
      "loss": 1.9784,
      "step": 47000
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 25.25235366821289,
      "learning_rate": 1.8334e-05,
      "loss": 2.1189,
      "step": 47500
    },
    {
      "epoch": 0.64,
      "grad_norm": 10.72451400756836,
      "learning_rate": 1.800066666666667e-05,
      "loss": 1.9399,
      "step": 48000
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 5.3897576332092285,
      "learning_rate": 1.7667333333333333e-05,
      "loss": 1.948,
      "step": 48500
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 6.220443248748779,
      "learning_rate": 1.7334e-05,
      "loss": 2.0331,
      "step": 49000
    },
    {
      "epoch": 0.66,
      "grad_norm": 10.533363342285156,
      "learning_rate": 1.7000666666666668e-05,
      "loss": 2.0256,
      "step": 49500
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 7.415234088897705,
      "learning_rate": 1.6667333333333336e-05,
      "loss": 2.0539,
      "step": 50000
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 12.504018783569336,
      "learning_rate": 1.6334000000000003e-05,
      "loss": 1.9483,
      "step": 50500
    },
    {
      "epoch": 0.68,
      "grad_norm": 8.138433456420898,
      "learning_rate": 1.6000666666666667e-05,
      "loss": 1.9999,
      "step": 51000
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 24.967235565185547,
      "learning_rate": 1.5667333333333335e-05,
      "loss": 1.8665,
      "step": 51500
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 9.019641876220703,
      "learning_rate": 1.5334000000000003e-05,
      "loss": 1.9788,
      "step": 52000
    },
    {
      "epoch": 0.7,
      "grad_norm": 33.598533630371094,
      "learning_rate": 1.5000666666666669e-05,
      "loss": 1.9022,
      "step": 52500
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 2.3132946491241455,
      "learning_rate": 1.4667333333333333e-05,
      "loss": 2.0314,
      "step": 53000
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 15.27712345123291,
      "learning_rate": 1.4334e-05,
      "loss": 1.8556,
      "step": 53500
    },
    {
      "epoch": 0.72,
      "grad_norm": 7.093460559844971,
      "learning_rate": 1.4000666666666668e-05,
      "loss": 1.9377,
      "step": 54000
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 6.848404884338379,
      "learning_rate": 1.3667333333333336e-05,
      "loss": 1.8983,
      "step": 54500
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 31.77239418029785,
      "learning_rate": 1.3334e-05,
      "loss": 1.9827,
      "step": 55000
    },
    {
      "epoch": 0.74,
      "grad_norm": 12.508014678955078,
      "learning_rate": 1.3000666666666667e-05,
      "loss": 1.8829,
      "step": 55500
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 9.26026439666748,
      "learning_rate": 1.2667333333333333e-05,
      "loss": 1.8753,
      "step": 56000
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 12.352561950683594,
      "learning_rate": 1.2334e-05,
      "loss": 1.8769,
      "step": 56500
    },
    {
      "epoch": 0.76,
      "grad_norm": 3.4364278316497803,
      "learning_rate": 1.2000666666666667e-05,
      "loss": 1.9535,
      "step": 57000
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 9.525390625,
      "learning_rate": 1.1667333333333334e-05,
      "loss": 1.8874,
      "step": 57500
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 17.87179946899414,
      "learning_rate": 1.1334e-05,
      "loss": 1.8817,
      "step": 58000
    },
    {
      "epoch": 0.78,
      "grad_norm": 7.366404056549072,
      "learning_rate": 1.1000666666666668e-05,
      "loss": 1.8331,
      "step": 58500
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 6.085831165313721,
      "learning_rate": 1.0667333333333334e-05,
      "loss": 1.8818,
      "step": 59000
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 6.380761623382568,
      "learning_rate": 1.0334000000000001e-05,
      "loss": 1.9068,
      "step": 59500
    },
    {
      "epoch": 0.8,
      "grad_norm": 8.020257949829102,
      "learning_rate": 1.0000666666666667e-05,
      "loss": 1.8762,
      "step": 60000
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 3.8833301067352295,
      "learning_rate": 9.667333333333333e-06,
      "loss": 1.9457,
      "step": 60500
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 6.123754501342773,
      "learning_rate": 9.334e-06,
      "loss": 1.8646,
      "step": 61000
    },
    {
      "epoch": 0.82,
      "grad_norm": 13.398421287536621,
      "learning_rate": 9.000666666666667e-06,
      "loss": 1.8078,
      "step": 61500
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 19.07552719116211,
      "learning_rate": 8.667333333333334e-06,
      "loss": 1.8167,
      "step": 62000
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 7.387587070465088,
      "learning_rate": 8.334e-06,
      "loss": 1.7956,
      "step": 62500
    },
    {
      "epoch": 0.84,
      "grad_norm": 3.6503288745880127,
      "learning_rate": 8.000666666666668e-06,
      "loss": 1.8579,
      "step": 63000
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 13.319544792175293,
      "learning_rate": 7.667333333333334e-06,
      "loss": 1.86,
      "step": 63500
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 13.091840744018555,
      "learning_rate": 7.3340000000000004e-06,
      "loss": 1.9473,
      "step": 64000
    },
    {
      "epoch": 0.86,
      "grad_norm": 14.471932411193848,
      "learning_rate": 7.000666666666666e-06,
      "loss": 1.7644,
      "step": 64500
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 3.6218183040618896,
      "learning_rate": 6.667333333333334e-06,
      "loss": 1.8643,
      "step": 65000
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 10.842085838317871,
      "learning_rate": 6.334e-06,
      "loss": 1.7757,
      "step": 65500
    },
    {
      "epoch": 0.88,
      "grad_norm": 8.086455345153809,
      "learning_rate": 6.000666666666667e-06,
      "loss": 1.8746,
      "step": 66000
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 8.696863174438477,
      "learning_rate": 5.667333333333333e-06,
      "loss": 1.89,
      "step": 66500
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 8.872529983520508,
      "learning_rate": 5.334e-06,
      "loss": 1.9041,
      "step": 67000
    },
    {
      "epoch": 0.9,
      "grad_norm": 2.7102158069610596,
      "learning_rate": 5.000666666666667e-06,
      "loss": 1.8656,
      "step": 67500
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 13.859971046447754,
      "learning_rate": 4.667333333333334e-06,
      "loss": 1.8138,
      "step": 68000
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 8.956266403198242,
      "learning_rate": 4.3339999999999995e-06,
      "loss": 1.8139,
      "step": 68500
    },
    {
      "epoch": 0.92,
      "grad_norm": 5.593354225158691,
      "learning_rate": 4.000666666666667e-06,
      "loss": 1.8502,
      "step": 69000
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 7.201134204864502,
      "learning_rate": 3.667333333333334e-06,
      "loss": 1.7748,
      "step": 69500
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 5.240952491760254,
      "learning_rate": 3.334e-06,
      "loss": 1.7865,
      "step": 70000
    },
    {
      "epoch": 0.94,
      "grad_norm": 9.666772842407227,
      "learning_rate": 3.000666666666667e-06,
      "loss": 1.8622,
      "step": 70500
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 3.071443557739258,
      "learning_rate": 2.6673333333333337e-06,
      "loss": 1.8233,
      "step": 71000
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 28.477563858032227,
      "learning_rate": 2.334e-06,
      "loss": 1.8822,
      "step": 71500
    },
    {
      "epoch": 0.96,
      "grad_norm": 9.969284057617188,
      "learning_rate": 2.0006666666666668e-06,
      "loss": 1.8705,
      "step": 72000
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 8.602144241333008,
      "learning_rate": 1.6673333333333333e-06,
      "loss": 1.8281,
      "step": 72500
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 9.084293365478516,
      "learning_rate": 1.334e-06,
      "loss": 1.7538,
      "step": 73000
    },
    {
      "epoch": 0.98,
      "grad_norm": 3.597040891647339,
      "learning_rate": 1.0006666666666668e-06,
      "loss": 1.8664,
      "step": 73500
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 13.338154792785645,
      "learning_rate": 6.673333333333334e-07,
      "loss": 1.828,
      "step": 74000
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 7.397912502288818,
      "learning_rate": 3.34e-07,
      "loss": 1.7418,
      "step": 74500
    },
    {
      "epoch": 1.0,
      "grad_norm": 9.525046348571777,
      "learning_rate": 6.666666666666667e-10,
      "loss": 1.764,
      "step": 75000
    }
  ],
  "logging_steps": 500,
  "max_steps": 75000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.1343683584e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
